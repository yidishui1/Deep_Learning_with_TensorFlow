{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 定义相关的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../datasets/PTB_data\"\n",
    "HIDDEN_SIZE = 200\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "LEARNING_RATE = 1.0\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "TRAIN_NUM_STEP = 35\n",
    "\n",
    "EVAL_BATCH_SIZE = 1\n",
    "EVAL_NUM_STEP = 1\n",
    "NUM_EPOCH = 2\n",
    "KEEP_PROB = 0.5\n",
    "MAX_GRAD_NORM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义一个类来描述模型结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        # 定义输入层。\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # 定义使用LSTM结构及训练时使用dropout。\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(HIDDEN_SIZE)\n",
    "        if is_training:\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=KEEP_PROB)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell]*NUM_LAYERS)\n",
    "        \n",
    "        # 初始化最初的状态。\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "        # 将原本单词ID转为单词向量。\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        \n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, KEEP_PROB)\n",
    "\n",
    "        # 定义输出列表。\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output) \n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    "        weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        \n",
    "        # 定义交叉熵损失函数和平均损失。\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [tf.reshape(self.targets, [-1])],\n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)])\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 只在训练模型时定义反向传播操作。\n",
    "        if not is_training: return\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "\n",
    "        # 控制梯度大小，定义优化方法和训练步骤。\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 使用给定的模型model在数据data上运行train_op并返回在全部数据上的perplexity值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, data, train_op, output_log, epoch_size):\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    # 训练一个epoch。\n",
    "    for step in range(epoch_size):\n",
    "        x, y = session.run(data)\n",
    "        cost, state, _ = session.run([model.cost, model.final_state, train_op],\n",
    "                                        {model.input_data: x, model.targets: y, model.initial_state: state})\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        if output_log and step % 100 == 0:\n",
    "            print(\"After %d steps, perplexity is %.3f\" % (step, np.exp(total_costs / iters)))\n",
    "    return np.exp(total_costs / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 定义主函数并执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: PTBProducer_1/input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](PTBProducer_1/input_producer, PTBProducer_1/input_producer/range)]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not flatten dictionary: key Tensor(\"language_model/MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0\", shape=(20, 200), dtype=float32) is not unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1cdc451087e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-1cdc451087e2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In iteration: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mvalid_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_epoch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7ff54e7b3b55>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(session, model, data, train_op, output_log, epoch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         cost, state, _ = session.run([model.cost, model.final_state, train_op],\n\u001b[0;32m---> 10\u001b[0;31m                                         {model.input_data: x, model.targets: y, model.initial_state: state})\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_costs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0miters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yujinyue/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yujinyue/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0mfeed_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_dict_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_feed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yujinyue/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/nest.pyc\u001b[0m in \u001b[0;36mflatten_dict_items\u001b[0;34m(dictionary)\u001b[0m\n\u001b[1;32m    249\u001b[0m           raise ValueError(\n\u001b[1;32m    250\u001b[0m               \u001b[0;34m\"Could not flatten dictionary: key %s is not unique.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m               % (new_i))\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mflat_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not flatten dictionary: key Tensor(\"language_model/MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0\", shape=(20, 200), dtype=float32) is not unique."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_data, valid_data, test_data, _ = reader.ptb_raw_data(DATA_PATH)\n",
    "\n",
    "    # 计算一个epoch需要训练的次数\n",
    "    train_data_len = len(train_data)\n",
    "    train_batch_len = train_data_len // TRAIN_BATCH_SIZE\n",
    "    train_epoch_size = (train_batch_len - 1) // TRAIN_NUM_STEP\n",
    "\n",
    "    valid_data_len = len(valid_data)\n",
    "    valid_batch_len = valid_data_len // EVAL_BATCH_SIZE\n",
    "    valid_epoch_size = (valid_batch_len - 1) // EVAL_NUM_STEP\n",
    "\n",
    "    test_data_len = len(test_data)\n",
    "    test_batch_len = test_data_len // EVAL_BATCH_SIZE\n",
    "    test_epoch_size = (test_batch_len - 1) // EVAL_NUM_STEP\n",
    "\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    with tf.variable_scope(\"language_model\", reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "\n",
    "    with tf.variable_scope(\"language_model\", reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "    # 训练模型。\n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        train_queue = reader.ptb_producer(train_data, train_model.batch_size, train_model.num_steps)\n",
    "        eval_queue = reader.ptb_producer(valid_data, eval_model.batch_size, eval_model.num_steps)\n",
    "        test_queue = reader.ptb_producer(test_data, eval_model.batch_size, eval_model.num_steps)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In iteration: %d\" % (i + 1))\n",
    "            run_epoch(session, train_model, train_queue, train_model.train_op, True, train_epoch_size)\n",
    "\n",
    "            valid_perplexity = run_epoch(session, eval_model, eval_queue, tf.no_op(), False, valid_epoch_size)\n",
    "            print(\"Epoch: %d Validation Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "        test_perplexity = run_epoch(session, eval_model, test_queue, tf.no_op(), False, test_epoch_size)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
